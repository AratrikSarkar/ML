{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYwZqDFjwf5AIkACVeaYZ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AratrikSarkar/ML/blob/main/new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_-462yomALq",
        "outputId": "aae5fca3-1191-49c9-caf0-e49117ed23b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | Loss 0.2022 | Acc 0.9121\n",
            "Epoch 2/50 | Loss 0.0198 | Acc 0.9939\n",
            "Epoch 3/50 | Loss 0.0129 | Acc 0.9962\n",
            "Epoch 4/50 | Loss 0.0089 | Acc 0.9975\n",
            "Epoch 5/50 | Loss 0.0065 | Acc 0.9982\n",
            "Epoch 6/50 | Loss 0.0060 | Acc 0.9984\n",
            "Epoch 7/50 | Loss 0.0052 | Acc 0.9986\n",
            "Epoch 8/50 | Loss 0.0037 | Acc 0.9990\n",
            "Epoch 9/50 | Loss 0.0034 | Acc 0.9991\n",
            "Epoch 10/50 | Loss 0.0032 | Acc 0.9992\n",
            "Epoch 11/50 | Loss 0.0034 | Acc 0.9991\n",
            "Epoch 12/50 | Loss 0.0031 | Acc 0.9992\n",
            "Epoch 13/50 | Loss 0.0001 | Acc 1.0000\n",
            "Epoch 14/50 | Loss 0.0017 | Acc 0.9996\n",
            "Epoch 15/50 | Loss 0.0037 | Acc 0.9990\n",
            "Epoch 16/50 | Loss 0.0030 | Acc 0.9992\n",
            "Epoch 17/50 | Loss 0.0025 | Acc 0.9994\n",
            "Epoch 18/50 | Loss 0.0032 | Acc 0.9992\n",
            "Epoch 19/50 | Loss 0.0027 | Acc 0.9993\n",
            "Epoch 20/50 | Loss 0.0022 | Acc 0.9994\n",
            "Epoch 21/50 | Loss 0.0032 | Acc 0.9992\n",
            "Epoch 22/50 | Loss 0.0015 | Acc 0.9996\n",
            "Epoch 23/50 | Loss 0.0026 | Acc 0.9994\n",
            "Epoch 24/50 | Loss 0.0013 | Acc 0.9997\n",
            "Epoch 25/50 | Loss 0.0017 | Acc 0.9996\n",
            "Epoch 26/50 | Loss 0.0012 | Acc 0.9997\n",
            "Epoch 27/50 | Loss 0.0000 | Acc 1.0000\n",
            "Epoch 28/50 | Loss 0.0011 | Acc 0.9998\n",
            "Epoch 29/50 | Loss 0.0023 | Acc 0.9994\n",
            "Epoch 30/50 | Loss 0.0023 | Acc 0.9995\n",
            "Epoch 31/50 | Loss 0.0016 | Acc 0.9996\n",
            "Epoch 32/50 | Loss 0.0017 | Acc 0.9996\n",
            "Epoch 33/50 | Loss 0.0011 | Acc 0.9997\n",
            "Epoch 34/50 | Loss 0.0014 | Acc 0.9997\n",
            "Epoch 35/50 | Loss 0.0025 | Acc 0.9994\n",
            "Epoch 36/50 | Loss 0.0018 | Acc 0.9996\n",
            "Epoch 37/50 | Loss 0.0012 | Acc 0.9997\n",
            "Epoch 38/50 | Loss 0.0019 | Acc 0.9996\n",
            "Epoch 39/50 | Loss 0.0018 | Acc 0.9996\n",
            "Epoch 40/50 | Loss 0.0016 | Acc 0.9996\n",
            "Epoch 41/50 | Loss 0.0015 | Acc 0.9997\n",
            "Epoch 42/50 | Loss 0.0017 | Acc 0.9996\n",
            "Epoch 43/50 | Loss 0.0014 | Acc 0.9996\n",
            "Epoch 44/50 | Loss 0.0017 | Acc 0.9996\n",
            "Epoch 45/50 | Loss 0.0016 | Acc 0.9997\n",
            "Epoch 46/50 | Loss 0.0012 | Acc 0.9997\n",
            "Epoch 47/50 | Loss 0.0009 | Acc 0.9998\n",
            "Epoch 48/50 | Loss 0.0011 | Acc 0.9998\n",
            "Epoch 49/50 | Loss 0.0014 | Acc 0.9997\n",
            "Epoch 50/50 | Loss 0.0012 | Acc 0.9997\n",
            "\n",
            "TEST FILE: BuEb\n",
            "Similarity: 0.7601900739176346\n",
            "Original: 18940\n",
            "Latent.txt: 179608 ratio: 0.10545187296779654\n",
            "Latent.pt : 20549 ratio: 0.921699352766558\n",
            "\n",
            "TEST FILE: AeCa\n",
            "Similarity: 0.7647859996769427\n",
            "Original: 1591049\n",
            "Latent.txt: 15152785 ratio: 0.1050004339136337\n",
            "Latent.pt : 1592645 ratio: 0.9989978934414135\n"
          ]
        }
      ],
      "source": [
        "from IPython.utils.path import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# ================= CONFIG =================\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BLOCK_SIZE = 8\n",
        "LATENT_DIM = 8\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "LR = 5e-4\n",
        "\n",
        "DNA_VOCAB = ['A','C','G','T','N']\n",
        "stoi = {c:i for i,c in enumerate(DNA_VOCAB)}\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "PAD = stoi['N']\n",
        "\n",
        "# ================= DATA =================\n",
        "def read_fasta(path):\n",
        "    seq = []\n",
        "    with open(path) as f:\n",
        "        for l in f:\n",
        "            l=l.strip().upper()\n",
        "            if l and not l.startswith(\">\"):\n",
        "                seq.append(l)\n",
        "    dna = \"\".join(seq)\n",
        "    return dna\n",
        "\n",
        "def encode_dna(seq):\n",
        "    return [stoi[c] for c in seq]\n",
        "\n",
        "def decode_dna(tokens):\n",
        "    return \"\".join(itos[t] for t in tokens)\n",
        "\n",
        "def chunk(tokens, size):\n",
        "    out=[]\n",
        "    for i in range(0,len(tokens),size):\n",
        "        b=tokens[i:i+size]\n",
        "        if len(b)<size: b=b+[PAD]*(size-len(b))\n",
        "        out.append(b)\n",
        "    return out\n",
        "\n",
        "# ================= MODEL =================\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(len(DNA_VOCAB),64)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(64,128,7,padding=3), nn.ReLU(),\n",
        "            nn.Conv1d(128,256,7,padding=3), nn.ReLU(),\n",
        "            nn.Conv1d(256,256,7,padding=3), nn.ReLU()\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(256, LATENT_DIM)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.emb(x).transpose(1,2)\n",
        "        x=self.conv(x)\n",
        "        x=self.pool(x).squeeze(-1)\n",
        "        return self.fc(x)\n",
        "\n",
        "class CNNDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(LATENT_DIM,256)\n",
        "        self.conv = nn.Sequential(\n",
        "          nn.Conv1d(256,256,7,padding=3), nn.ReLU(),\n",
        "          nn.Conv1d(256,256,7,padding=3), nn.ReLU(),\n",
        "          nn.Conv1d(256,128,7,padding=3), nn.ReLU(),\n",
        "          nn.Conv1d(128,len(DNA_VOCAB),1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self,z,seq_len):\n",
        "        x=self.fc(z).unsqueeze(-1).repeat(1,1,seq_len)\n",
        "        x=self.conv(x)\n",
        "        return x.transpose(1,2)\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.enc=CNNEncoder()\n",
        "        self.dec=CNNDecoder()\n",
        "    def forward(self,x):\n",
        "        z=self.enc(x)\n",
        "        return self.dec(z,x.size(1))\n",
        "\n",
        "# ================= LOSS =================\n",
        "def loss_fn(logits,targets):\n",
        "    return F.cross_entropy(\n",
        "        logits.reshape(-1,logits.size(-1)),\n",
        "        targets.reshape(-1),\n",
        "        ignore_index=PAD\n",
        "    )\n",
        "\n",
        "#======================ACCURACY FUNCTION========\n",
        "def accuracy_fn(logits, targets):\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "    mask = (targets != PAD)\n",
        "    correct = (preds == targets) & mask\n",
        "    return (correct.sum().float() / mask.sum().float()).item()\n",
        "\n",
        "# ================= TRAIN FROM data/ =================\n",
        "TRAIN_DIR = \"data\"\n",
        "TEST_DIR = \"testing\"\n",
        "OUT_DIR = \"outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Load training files ----\n",
        "train_tokens = []\n",
        "for fname in os.listdir(TRAIN_DIR):\n",
        "    path = os.path.join(TRAIN_DIR, fname)\n",
        "    if not os.path.isfile(path):\n",
        "        continue\n",
        "    dna = read_fasta(path)\n",
        "    tokens = encode_dna(dna)\n",
        "    train_tokens.extend(tokens)\n",
        "\n",
        "if len(train_tokens) == 0:\n",
        "    raise RuntimeError(\"No training data found in data/\")\n",
        "\n",
        "blocks = chunk(train_tokens, BLOCK_SIZE)\n",
        "train_data = torch.tensor(blocks, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "model = AutoEncoder().to(DEVICE)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# ===== TRAIN =====\n",
        "for e in range(EPOCHS):\n",
        "    model.train()\n",
        "    perm = torch.randperm(train_data.size(0))\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    steps = 0\n",
        "\n",
        "    for i in range(0, train_data.size(0), BATCH_SIZE):\n",
        "        batch = train_data[perm[i:i+BATCH_SIZE]]\n",
        "        out = model(batch)\n",
        "        loss = loss_fn(out, batch)\n",
        "        acc = accuracy_fn(out, batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += acc\n",
        "        steps += 1\n",
        "\n",
        "    print(f\"Epoch {e+1}/{EPOCHS} | Loss {total_loss/steps:.4f} | Acc {total_acc/steps:.4f}\")\n",
        "\n",
        "# ================= TEST FROM testing/ =================\n",
        "model.eval()\n",
        "for fname in os.listdir(TEST_DIR):\n",
        "    path = os.path.join(TEST_DIR, fname)\n",
        "    if not os.path.isfile(path):\n",
        "        continue\n",
        "\n",
        "    dna = read_fasta(path)\n",
        "    tokens = encode_dna(dna)\n",
        "    blocks = chunk(tokens, BLOCK_SIZE)\n",
        "    test_data = torch.tensor(blocks, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "    base = os.path.splitext(fname)[0]\n",
        "\n",
        "    # ---- Save encoded ----\n",
        "    with open(f\"{OUT_DIR}/{base}_encoded.txt\",\"w\") as f:\n",
        "        f.write(\" \".join(map(str,tokens)))\n",
        "\n",
        "    # ---- Save latents ----\n",
        "    latents=[]\n",
        "    with torch.no_grad():\n",
        "        for i in range(test_data.size(0)):\n",
        "            z=model.enc(test_data[i:i+1])\n",
        "            latents.append(z.cpu())\n",
        "    latents=torch.cat(latents,0)\n",
        "    latents_q=(latents*127).round().clamp(-128,127).to(torch.int8)\n",
        "\n",
        "    torch.save(latents_q, f\"{OUT_DIR}/{base}_latent.pt\")\n",
        "    loaded_latents_q = torch.load(f\"{OUT_DIR}/{base}_latent.pt\")\n",
        "    loaded_latents = loaded_latents_q.float()/127.0\n",
        "\n",
        "    with open(f\"{OUT_DIR}/{base}_latent.txt\",\"w\") as f:\n",
        "        for row in loaded_latents:\n",
        "            f.write(\",\".join(f\"{v:.6f}\" for v in row.tolist())+\"\\n\")\n",
        "\n",
        "    # ---- Reconstruct ----\n",
        "    recon=[]\n",
        "    with torch.no_grad():\n",
        "        for i in range(0,loaded_latents.size(0),BATCH_SIZE):\n",
        "            batch = loaded_latents[i:i+BATCH_SIZE].to(DEVICE)\n",
        "            out=model.dec(batch, BLOCK_SIZE)\n",
        "            pred=torch.argmax(out,-1).cpu().tolist()\n",
        "            for b in pred: recon.extend(b)\n",
        "\n",
        "    recon=recon[:len(tokens)]\n",
        "    recon_text=decode_dna(recon)\n",
        "\n",
        "    with open(f\"{OUT_DIR}/{base}_reconstructed.txt\",\"w\") as f:\n",
        "        f.write(recon_text)\n",
        "\n",
        "    # ---- Metrics ----\n",
        "    same=sum(a==b for a,b in zip(dna,recon_text))\n",
        "    sim=same/len(dna)\n",
        "\n",
        "    orig_size=len(dna)\n",
        "    latent_txt_size=os.path.getsize(f\"{OUT_DIR}/{base}_latent.txt\")\n",
        "    latent_pt_size=os.path.getsize(f\"{OUT_DIR}/{base}_latent.pt\")\n",
        "\n",
        "    print(f\"\\nTEST FILE: {fname}\")\n",
        "    print(\"Similarity:\", sim)\n",
        "    print(\"Original:\", orig_size)\n",
        "    print(\"Latent.txt:\", latent_txt_size, \"ratio:\", orig_size/latent_txt_size)\n",
        "    print(\"Latent.pt :\", latent_pt_size, \"ratio:\", orig_size/latent_pt_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hr2lnzFVn8mr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}