{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mfyYm8M2bWu",
        "outputId": "8ee30060-3abe-4ba8-b5c0-e57877fb44d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | Loss 0.2850 | Acc 0.8657\n",
            "Epoch 2/50 | Loss 0.0722 | Acc 0.9694\n",
            "Epoch 3/50 | Loss 0.0375 | Acc 0.9858\n",
            "Epoch 4/50 | Loss 0.0223 | Acc 0.9925\n",
            "Epoch 5/50 | Loss 0.0140 | Acc 0.9957\n",
            "Epoch 6/50 | Loss 0.0106 | Acc 0.9969\n",
            "Epoch 7/50 | Loss 0.0084 | Acc 0.9976\n",
            "Epoch 8/50 | Loss 0.0069 | Acc 0.9981\n",
            "Epoch 9/50 | Loss 0.0060 | Acc 0.9984\n",
            "Epoch 10/50 | Loss 0.0052 | Acc 0.9986\n",
            "Epoch 11/50 | Loss 0.0047 | Acc 0.9988\n",
            "Epoch 12/50 | Loss 0.0044 | Acc 0.9989\n",
            "Epoch 13/50 | Loss 0.0040 | Acc 0.9990\n",
            "Epoch 14/50 | Loss 0.0036 | Acc 0.9991\n",
            "Epoch 15/50 | Loss 0.0035 | Acc 0.9992\n",
            "Epoch 16/50 | Loss 0.0033 | Acc 0.9992\n",
            "Epoch 17/50 | Loss 0.0031 | Acc 0.9993\n",
            "Epoch 18/50 | Loss 0.0029 | Acc 0.9993\n",
            "Epoch 19/50 | Loss 0.0029 | Acc 0.9994\n",
            "Epoch 20/50 | Loss 0.0028 | Acc 0.9994\n",
            "Epoch 21/50 | Loss 0.0030 | Acc 0.9994\n",
            "Epoch 22/50 | Loss 0.0027 | Acc 0.9994\n",
            "Epoch 23/50 | Loss 0.0032 | Acc 0.9994\n",
            "Epoch 24/50 | Loss 0.0027 | Acc 0.9994\n",
            "Epoch 25/50 | Loss 0.0037 | Acc 0.9994\n",
            "Epoch 26/50 | Loss 0.0029 | Acc 0.9994\n",
            "Epoch 27/50 | Loss 0.0027 | Acc 0.9994\n",
            "Epoch 28/50 | Loss 0.0033 | Acc 0.9994\n",
            "Epoch 29/50 | Loss 0.0026 | Acc 0.9994\n",
            "Epoch 30/50 | Loss 0.0027 | Acc 0.9994\n",
            "Epoch 31/50 | Loss 0.0029 | Acc 0.9994\n",
            "Epoch 32/50 | Loss 0.0031 | Acc 0.9994\n",
            "Epoch 33/50 | Loss 0.0028 | Acc 0.9994\n",
            "Epoch 34/50 | Loss 0.0072 | Acc 0.9995\n",
            "Epoch 35/50 | Loss 0.0027 | Acc 0.9995\n",
            "Epoch 36/50 | Loss 0.0027 | Acc 0.9995\n",
            "Epoch 37/50 | Loss 0.0027 | Acc 0.9995\n",
            "Epoch 38/50 | Loss 0.0194 | Acc 0.9995\n",
            "Epoch 39/50 | Loss 0.0030 | Acc 0.9995\n",
            "Epoch 40/50 | Loss 0.0028 | Acc 0.9995\n",
            "Epoch 41/50 | Loss 0.0027 | Acc 0.9995\n",
            "Epoch 42/50 | Loss 0.0029 | Acc 0.9995\n",
            "Epoch 43/50 | Loss 0.0042 | Acc 0.9995\n",
            "Epoch 44/50 | Loss 0.0398 | Acc 0.9993\n",
            "Epoch 45/50 | Loss 0.0028 | Acc 0.9995\n",
            "Epoch 46/50 | Loss 0.0030 | Acc 0.9995\n",
            "Epoch 47/50 | Loss 0.0035 | Acc 0.9994\n",
            "Epoch 48/50 | Loss 0.0031 | Acc 0.9995\n",
            "Epoch 49/50 | Loss 0.0034 | Acc 0.9995\n",
            "Epoch 50/50 | Loss 0.0031 | Acc 0.9995\n",
            "\n",
            "TEST FILE: AeCa\n",
            "Similarity: 0.9996373461785275\n",
            "Original: 1591049\n",
            "Latent.txt: 15111315 ratio: 0.10528858673120109\n",
            "Latent.pt : 1592645 ratio: 0.9989978934414135\n",
            "\n",
            "TEST FILE: OrSa\n",
            "Similarity: 0.9996854783527073\n",
            "Original: 43262523\n",
            "Latent.txt: 412337212 ratio: 0.10492024910911994\n",
            "Latent.pt : 43264133 ratio: 0.9999627867268251\n",
            "\n",
            "TEST FILE: ScPo\n",
            "Similarity: 0.9997065382544659\n",
            "Original: 10652155\n",
            "Latent.txt: 101692248 ratio: 0.10474893818848414\n",
            "Latent.pt : 10653765 ratio: 0.9998488797152931\n",
            "\n",
            "TEST FILE: HePy\n",
            "Similarity: 0.9996690300241332\n",
            "Original: 1667825\n",
            "Latent.txt: 15914681 ratio: 0.10479789070230186\n",
            "Latent.pt : 1669445 ratio: 0.999029617627415\n",
            "\n",
            "TEST FILE: PlFa\n",
            "Similarity: 0.9997400606584477\n",
            "Original: 8986712\n",
            "Latent.txt: 86010257 ratio: 0.10448418960078215\n",
            "Latent.pt : 8988293 ratio: 0.9998241045324179\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# ================= CONFIG =================\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BLOCK_SIZE = 16\n",
        "LATENT_DIM = 16\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "LR = 5e-4\n",
        "\n",
        "DNA_VOCAB = ['A','C','G','T','N']\n",
        "stoi = {c:i for i,c in enumerate(DNA_VOCAB)}\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "PAD = stoi['N']\n",
        "\n",
        "# ================= DATA =================\n",
        "def read_fasta(path):\n",
        "    seq = []\n",
        "    with open(path) as f:\n",
        "        for l in f:\n",
        "            l=l.strip().upper()\n",
        "            if l and not l.startswith(\">\"):\n",
        "                seq.append(l)\n",
        "    dna = \"\".join(seq)\n",
        "    return dna\n",
        "\n",
        "def encode_dna(seq):\n",
        "    return [stoi[c] for c in seq]\n",
        "\n",
        "def decode_dna(tokens):\n",
        "    return \"\".join(itos[t] for t in tokens)\n",
        "\n",
        "def chunk(tokens, size):\n",
        "    out=[]\n",
        "    for i in range(0,len(tokens),size):\n",
        "        b=tokens[i:i+size]\n",
        "        if len(b)<size: b=b+[PAD]*(size-len(b))\n",
        "        out.append(b)\n",
        "    return out\n",
        "\n",
        "# ================= MODEL =================\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(len(DNA_VOCAB),64)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(64,128,7,padding=3), nn.ReLU(),\n",
        "            nn.Conv1d(128,256,7,padding=3), nn.ReLU(),\n",
        "            nn.Conv1d(256,256,7,padding=3), nn.ReLU()\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(256, LATENT_DIM)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.emb(x).transpose(1,2)\n",
        "        x=self.conv(x)\n",
        "        x=self.pool(x).squeeze(-1)\n",
        "        return self.fc(x)\n",
        "\n",
        "class CNNDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(LATENT_DIM,256)\n",
        "        self.conv = nn.Sequential(\n",
        "          nn.Conv1d(256,256,7,padding=3), nn.ReLU(),\n",
        "          nn.Conv1d(256,256,7,padding=3), nn.ReLU(),\n",
        "          nn.Conv1d(256,128,7,padding=3), nn.ReLU(),\n",
        "          nn.Conv1d(128,len(DNA_VOCAB),1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self,z,seq_len):\n",
        "        x=self.fc(z).unsqueeze(-1).repeat(1,1,seq_len)\n",
        "        x=self.conv(x)\n",
        "        return x.transpose(1,2)\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.enc=CNNEncoder()\n",
        "        self.dec=CNNDecoder()\n",
        "    def forward(self,x):\n",
        "        z=self.enc(x)\n",
        "        return self.dec(z,x.size(1))\n",
        "\n",
        "# ================= LOSS =================\n",
        "def loss_fn(logits,targets):\n",
        "    return F.cross_entropy(\n",
        "        logits.reshape(-1,logits.size(-1)),\n",
        "        targets.reshape(-1),\n",
        "        ignore_index=PAD\n",
        "    )\n",
        "\n",
        "#======================ACCURACY FUNCTION========\n",
        "def accuracy_fn(logits, targets):\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "    mask = (targets != PAD)\n",
        "    correct = (preds == targets) & mask\n",
        "    return (correct.sum().float() / mask.sum().float()).item()\n",
        "\n",
        "\n",
        "# # ================= MULTI-FILE MAIN =================\n",
        "# INPUT_DIR = \"data\"\n",
        "# OUTPUT_DIR = \"outputs\"\n",
        "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# # ---- Load all files for training ----\n",
        "# all_tokens = []\n",
        "# file_data = {}   # store per-file tokens\n",
        "\n",
        "# for fname in os.listdir(INPUT_DIR):\n",
        "#     path = os.path.join(INPUT_DIR, fname)\n",
        "#     if os.path.isfile(path): # Add this line to check if it's a file\n",
        "#         dna = read_fasta(path)\n",
        "#         tokens = encode_dna(dna)\n",
        "#         file_data[fname] = (dna, tokens)\n",
        "#         all_tokens.extend(tokens)\n",
        "\n",
        "# # ---- Build dataset from all files ----\n",
        "# blocks = chunk(all_tokens, BLOCK_SIZE)\n",
        "# data = torch.tensor(blocks, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "# model = AutoEncoder().to(DEVICE)\n",
        "# opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# # ===== TRAIN =====\n",
        "# for e in range(EPOCHS):\n",
        "#     model.train()\n",
        "#     perm = torch.randperm(data.size(0))\n",
        "#     total_loss = 0\n",
        "#     total_acc = 0\n",
        "#     steps = 0\n",
        "\n",
        "#     for i in range(0, data.size(0), BATCH_SIZE):\n",
        "#         batch = data[perm[i:i+BATCH_SIZE]]\n",
        "#         out = model(batch)\n",
        "#         loss = loss_fn(out, batch)\n",
        "#         acc = accuracy_fn(out, batch)\n",
        "\n",
        "#         opt.zero_grad()\n",
        "#         loss.backward()\n",
        "#         opt.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "#         total_acc += acc\n",
        "#         steps += 1\n",
        "\n",
        "#     print(f\"Epoch {e+1}/{EPOCHS} | Loss {total_loss/steps:.4f} | Acc {total_acc/steps:.4f}\")\n",
        "\n",
        "# # ===== PROCESS EACH FILE SEPARATELY =====\n",
        "# model.eval()\n",
        "# for fname, (dna, tokens) in file_data.items():\n",
        "#     blocks = chunk(tokens, BLOCK_SIZE)\n",
        "#     data_f = torch.tensor(blocks, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "#     base = os.path.splitext(fname)[0]\n",
        "\n",
        "#     # ---- Save encoded ----\n",
        "#     with open(f\"{OUTPUT_DIR}/{base}_encoded.txt\",\"w\") as f:\n",
        "#         f.write(\" \".join(map(str,tokens)))\n",
        "\n",
        "#     # ---- Save latents ----\n",
        "#     latents=[]\n",
        "#     with torch.no_grad():\n",
        "#         for i in range(data_f.size(0)):\n",
        "#             z=model.enc(data_f[i:i+1])\n",
        "#             latents.append(z.cpu())\n",
        "#     latents=torch.cat(latents,0)\n",
        "#     latents_q=(latents*127).clamp(-128,127).to(torch.int8)\n",
        "\n",
        "#     torch.save(latents_q, f\"{OUTPUT_DIR}/{base}_latent.pt\")\n",
        "\n",
        "#     with open(f\"{OUTPUT_DIR}/{base}_latent.txt\",\"w\") as f:\n",
        "#         for row in latents:\n",
        "#             f.write(\",\".join(f\"{v:.6f}\" for v in row.tolist())+\"\\n\")\n",
        "\n",
        "#     # ---- Reconstruct ----\n",
        "#     recon=[]\n",
        "#     with torch.no_grad():\n",
        "#         for i in range(0,data_f.size(0),BATCH_SIZE):\n",
        "#             batch=data_f[i:i+BATCH_SIZE]\n",
        "#             out=model(batch)\n",
        "#             pred=torch.argmax(out,-1).cpu().tolist()\n",
        "#             for b in pred: recon.extend(b)\n",
        "\n",
        "#     recon=recon[:len(tokens)]\n",
        "#     recon_text=decode_dna(recon)\n",
        "\n",
        "#     with open(f\"{OUTPUT_DIR}/{base}_reconstructed.txt\",\"w\") as f:\n",
        "#         f.write(recon_text)\n",
        "\n",
        "#     # ---- Metrics ----\n",
        "#     same=sum(a==b for a,b in zip(dna,recon_text))\n",
        "#     sim=same/len(dna)\n",
        "\n",
        "#     orig_size=len(dna)\n",
        "#     latent_txt_size=os.path.getsize(f\"{OUTPUT_DIR}/{base}_latent.txt\")\n",
        "#     latent_pt_size=os.path.getsize(f\"{OUTPUT_DIR}/{base}_latent.pt\")\n",
        "\n",
        "#     print(f\"\\nFile: {fname}\")\n",
        "#     print(\"Similarity:\", sim)\n",
        "#     print(\"Original:\", orig_size)\n",
        "#     print(\"Latent.txt:\", latent_txt_size, \"ratio:\", orig_size/latent_txt_size)\n",
        "#     print(\"Latent.pt :\", latent_pt_size, \"ratio:\", orig_size/latent_pt_size)\n",
        "\n",
        "# ================= TRAIN FROM data/ =================\n",
        "TRAIN_DIR = \"data\"\n",
        "TEST_DIR = \"testing\"\n",
        "OUT_DIR = \"outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Load training files ----\n",
        "train_tokens = []\n",
        "for fname in os.listdir(TRAIN_DIR):\n",
        "    path = os.path.join(TRAIN_DIR, fname)\n",
        "    if not os.path.isfile(path):\n",
        "        continue\n",
        "    dna = read_fasta(path)\n",
        "    tokens = encode_dna(dna)\n",
        "    train_tokens.extend(tokens)\n",
        "\n",
        "if len(train_tokens) == 0:\n",
        "    raise RuntimeError(\"No training data found in data/\")\n",
        "\n",
        "blocks = chunk(train_tokens, BLOCK_SIZE)\n",
        "train_data = torch.tensor(blocks, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "model = AutoEncoder().to(DEVICE)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# ===== TRAIN =====\n",
        "for e in range(EPOCHS):\n",
        "    model.train()\n",
        "    perm = torch.randperm(train_data.size(0))\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    steps = 0\n",
        "\n",
        "    for i in range(0, train_data.size(0), BATCH_SIZE):\n",
        "        batch = train_data[perm[i:i+BATCH_SIZE]]\n",
        "        out = model(batch)\n",
        "        loss = loss_fn(out, batch)\n",
        "        acc = accuracy_fn(out, batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += acc\n",
        "        steps += 1\n",
        "\n",
        "    print(f\"Epoch {e+1}/{EPOCHS} | Loss {total_loss/steps:.4f} | Acc {total_acc/steps:.4f}\")\n",
        "\n",
        "# ================= TEST FROM testing/ =================\n",
        "model.eval()\n",
        "for fname in os.listdir(TEST_DIR):\n",
        "    path = os.path.join(TEST_DIR, fname)\n",
        "    if not os.path.isfile(path):\n",
        "        continue\n",
        "\n",
        "    dna = read_fasta(path)\n",
        "    tokens = encode_dna(dna)\n",
        "    blocks = chunk(tokens, BLOCK_SIZE)\n",
        "    test_data = torch.tensor(blocks, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "    base = os.path.splitext(fname)[0]\n",
        "\n",
        "    # ---- Save encoded ----\n",
        "    with open(f\"{OUT_DIR}/{base}_encoded.txt\",\"w\") as f:\n",
        "        f.write(\" \".join(map(str,tokens)))\n",
        "\n",
        "    # ---- Save latents ----\n",
        "    latents=[]\n",
        "    with torch.no_grad():\n",
        "        for i in range(test_data.size(0)):\n",
        "            z=model.enc(test_data[i:i+1])\n",
        "            latents.append(z.cpu())\n",
        "    latents=torch.cat(latents,0)\n",
        "    latents_q=(latents*127).clamp(-128,127).to(torch.int8)\n",
        "\n",
        "    torch.save(latents_q, f\"{OUT_DIR}/{base}_latent.pt\")\n",
        "\n",
        "    with open(f\"{OUT_DIR}/{base}_latent.txt\",\"w\") as f:\n",
        "        for row in latents:\n",
        "            f.write(\",\".join(f\"{v:.6f}\" for v in row.tolist())+\"\\n\")\n",
        "\n",
        "    # ---- Reconstruct ----\n",
        "    recon=[]\n",
        "    with torch.no_grad():\n",
        "        for i in range(0,test_data.size(0),BATCH_SIZE):\n",
        "            batch=test_data[i:i+BATCH_SIZE]\n",
        "            out=model(batch)\n",
        "            pred=torch.argmax(out,-1).cpu().tolist()\n",
        "            for b in pred: recon.extend(b)\n",
        "\n",
        "    recon=recon[:len(tokens)]\n",
        "    recon_text=decode_dna(recon)\n",
        "\n",
        "    with open(f\"{OUT_DIR}/{base}_reconstructed.txt\",\"w\") as f:\n",
        "        f.write(recon_text)\n",
        "\n",
        "    # ---- Metrics ----\n",
        "    same=sum(a==b for a,b in zip(dna,recon_text))\n",
        "    sim=same/len(dna)\n",
        "\n",
        "    orig_size=len(dna)\n",
        "    latent_txt_size=os.path.getsize(f\"{OUT_DIR}/{base}_latent.txt\")\n",
        "    latent_pt_size=os.path.getsize(f\"{OUT_DIR}/{base}_latent.pt\")\n",
        "\n",
        "    print(f\"\\nTEST FILE: {fname}\")\n",
        "    print(\"Similarity:\", sim)\n",
        "    print(\"Original:\", orig_size)\n",
        "    print(\"Latent.txt:\", latent_txt_size, \"ratio:\", orig_size/latent_txt_size)\n",
        "    print(\"Latent.pt :\", latent_pt_size, \"ratio:\", orig_size/latent_pt_size)"
      ]
    }
  ]
}