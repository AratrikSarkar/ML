{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EJfiqzCJcci7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# DNA Vocabulary\n",
        "DNA_VOCAB = ['A', 'C', 'G', 'T', 'N']\n",
        "stoi = {ch: i for i, ch in enumerate(DNA_VOCAB)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "PAD_TOKEN = stoi['N']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#different types of configurations\n",
        "CONFIG = {\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "\n",
        "    \"data\": {\n",
        "        \"block_size\": 128\n",
        "    },\n",
        "\n",
        "    \"training\": {\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 30,\n",
        "        \"learning_rate\": 3e-4\n",
        "    },\n",
        "\n",
        "    \"model\": {\n",
        "        \"vocab_size\": len(DNA_VOCAB),\n",
        "        \"latent_dim\": 64,\n",
        "\n",
        "        # \"encoder\": {\n",
        "        #     \"type\": \"lstm\",\n",
        "        #     \"emb_dim\": 64,\n",
        "        #     \"hidden_dim\": 128,\n",
        "        #     \"num_heads\": 4,\n",
        "        #     \"kernel_sizes\": [3, 5, 7]\n",
        "        # },\n",
        "\n",
        "        # \"decoder\": {\n",
        "        #     \"type\": \"lstm\",\n",
        "        #     \"hidden_dim\": 128,\n",
        "        #     \"num_heads\": 4\n",
        "        # }\n",
        "        # \"encoder\": {\n",
        "        #     \"type\": \"cnn_v2\",\n",
        "        #     \"emb_dim\": 64\n",
        "        # },\n",
        "        # \"decoder\": {\n",
        "        #     \"type\": \"cnn\"\n",
        "        # }\n",
        "\n",
        "      \"encoder\": {\n",
        "          \"type\": \"transformer\",\n",
        "          \"emb_dim\": 128,\n",
        "          \"num_heads\": 8,\n",
        "          \"num_layers\": 4\n",
        "      },\n",
        "      \"decoder\": {\n",
        "          \"type\": \"transformer\",\n",
        "          \"emb_dim\": 128,\n",
        "          \"num_heads\": 8,\n",
        "          \"num_layers\": 4\n",
        "      }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "KVzzGR5_t501"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base encoder class for other encoders\n",
        "class BaseEncoder(nn.Module):\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Input:  x -> (B, L)\n",
        "        Output: z -> (B, latent_dim)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n"
      ],
      "metadata": {
        "id": "Ddh6txcrtUA1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMEncoder(BaseEncoder):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, latent_dim, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            emb_dim,\n",
        "            hidden_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            2 * hidden_dim,\n",
        "            num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc_latent = nn.Linear(2 * hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)                # (B, L, E)\n",
        "        out, _ = self.lstm(x)                # (B, L, 2H)\n",
        "        attn, _ = self.attention(out, out, out)\n",
        "        pooled = attn.mean(dim=1)\n",
        "        return self.fc_latent(pooled)\n"
      ],
      "metadata": {
        "id": "sPECSWS5tXAl"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNEncoder(BaseEncoder):\n",
        "    def __init__(self, vocab_size, emb_dim, latent_dim, kernel_sizes=[3,5,7]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(\n",
        "                in_channels=emb_dim,\n",
        "                out_channels=128,\n",
        "                kernel_size=k,\n",
        "                padding=k//2\n",
        "            )\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        self.fc_latent = nn.Linear(128 * len(kernel_sizes), latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)       # (B, L, E)\n",
        "        x = x.transpose(1, 2)       # (B, E, L)\n",
        "\n",
        "        feats = []\n",
        "        for conv in self.convs:\n",
        "            f = F.relu(conv(x))\n",
        "            f = f.mean(dim=2)       # global avg pooling\n",
        "            feats.append(f)\n",
        "\n",
        "        feats = torch.cat(feats, dim=1)\n",
        "        return self.fc_latent(feats)\n"
      ],
      "metadata": {
        "id": "Od_Wji_1tZfV"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNEncoderV2(BaseEncoder):\n",
        "    def __init__(self, vocab_size, emb_dim, latent_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(emb_dim, 128, 7, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 256, 7, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(256, 256, 7, padding=3),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(256, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).transpose(1, 2)\n",
        "        x = self.conv(x)\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "4LCsSerd4JPs"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(BaseEncoder):\n",
        "    def __init__(self, vocab_size, emb_dim, latent_dim, n_heads, n_layers):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=emb_dim,\n",
        "            nhead=n_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
        "        self.fc = nn.Linear(emb_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = self.transformer(x)\n",
        "        return self.fc(x.mean(dim=1))\n"
      ],
      "metadata": {
        "id": "K6vqLHcx3pVt"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base decoder class for other decoders\n",
        "class BaseDecoder(nn.Module):\n",
        "    def forward(self, z, seq_len):\n",
        "        \"\"\"\n",
        "        Input:  z -> (B, latent_dim)\n",
        "        Output: logits -> (B, L, vocab)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n"
      ],
      "metadata": {
        "id": "kZl0rLQDtcJ0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMDecoder(BaseDecoder):\n",
        "    def __init__(self, vocab_size, latent_dim, hidden_dim, num_heads, max_len):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pos_embedding = nn.Embedding(max_len, latent_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            latent_dim,\n",
        "            hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            hidden_dim,\n",
        "            num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, z, seq_len):\n",
        "        B = z.size(0)\n",
        "\n",
        "        pos = torch.arange(seq_len, device=z.device)\n",
        "        pos_emb = self.pos_embedding(pos).unsqueeze(0).repeat(B, 1, 1)\n",
        "\n",
        "        z = z.unsqueeze(1).repeat(1, seq_len, 1) + pos_emb\n",
        "        out, _ = self.lstm(z)\n",
        "        out, _ = self.attention(out, out, out)\n",
        "        return self.fc_out(out)\n"
      ],
      "metadata": {
        "id": "J5mIYhXPtev9"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNDecoder(BaseDecoder):\n",
        "    def __init__(self, vocab_size, latent_dim, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.fc = nn.Linear(latent_dim, 256)\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(256, 256, 7, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(256, 128, 7, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, vocab_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, seq_len):\n",
        "        x = self.fc(z).unsqueeze(-1).repeat(1, 1, seq_len)\n",
        "        x = self.conv(x)\n",
        "        return x.transpose(1, 2)\n"
      ],
      "metadata": {
        "id": "YvUe2IvG3f3F"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(BaseDecoder):\n",
        "    def __init__(self, vocab_size, latent_dim, emb_dim, n_heads, n_layers, max_len):\n",
        "        super().__init__()\n",
        "        self.pos = nn.Embedding(max_len, emb_dim)\n",
        "        self.fc = nn.Linear(latent_dim, emb_dim)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=emb_dim,\n",
        "            nhead=n_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, n_layers)\n",
        "        self.out = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "    def forward(self, z, seq_len):\n",
        "        B = z.size(0)\n",
        "        tgt = self.fc(z).unsqueeze(1).repeat(1, seq_len, 1)\n",
        "        pos = self.pos(torch.arange(seq_len, device=z.device)).unsqueeze(0)\n",
        "        tgt = tgt + pos\n",
        "        out = self.decoder(tgt, tgt)\n",
        "        return self.out(out)\n"
      ],
      "metadata": {
        "id": "V6_xZK0Q3sGt"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#auto encoder class\n",
        "class DNAAutoEncoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, seq_len):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z, self.seq_len)\n"
      ],
      "metadata": {
        "id": "8c465dVIthV1"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#choosing encoder\n",
        "def build_encoder(cfg):\n",
        "    enc = cfg[\"model\"][\"encoder\"]\n",
        "    vocab = cfg[\"model\"][\"vocab_size\"]\n",
        "    latent = cfg[\"model\"][\"latent_dim\"]\n",
        "\n",
        "    if enc[\"type\"] == \"lstm\":\n",
        "        return LSTMEncoder(\n",
        "            vocab_size=vocab,\n",
        "            emb_dim=enc[\"emb_dim\"],\n",
        "            hidden_dim=enc[\"hidden_dim\"],\n",
        "            latent_dim=latent,\n",
        "            num_heads=enc[\"num_heads\"]\n",
        "        )\n",
        "\n",
        "    elif enc[\"type\"] == \"cnn\":\n",
        "        return CNNEncoder(\n",
        "            vocab_size=vocab,\n",
        "            emb_dim=enc[\"emb_dim\"],\n",
        "            latent_dim=latent,\n",
        "            kernel_sizes=enc[\"kernel_sizes\"]\n",
        "        )\n",
        "\n",
        "    elif enc[\"type\"] == \"cnn_v2\":\n",
        "        return CNNEncoderV2(\n",
        "            vocab_size=vocab,\n",
        "            emb_dim=enc[\"emb_dim\"],\n",
        "            latent_dim=latent\n",
        "        )\n",
        "\n",
        "    elif enc[\"type\"] == \"transformer\":\n",
        "        return TransformerEncoder(\n",
        "            vocab_size=vocab,\n",
        "            emb_dim=enc[\"emb_dim\"],\n",
        "            latent_dim=latent,\n",
        "            n_heads=enc[\"num_heads\"],\n",
        "            n_layers=enc[\"num_layers\"]\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown encoder type: {enc['type']}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PxGL1ONMuBGE"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O2siKDIrvlt1"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#choosing decoder\n",
        "def build_decoder(cfg):\n",
        "    dec = cfg[\"model\"][\"decoder\"]\n",
        "    vocab = cfg[\"model\"][\"vocab_size\"]\n",
        "    latent = cfg[\"model\"][\"latent_dim\"]\n",
        "    seq_len = cfg[\"data\"][\"block_size\"]\n",
        "\n",
        "    if dec[\"type\"] == \"lstm\":\n",
        "        return LSTMDecoder(\n",
        "            vocab_size=vocab,\n",
        "            latent_dim=latent,\n",
        "            hidden_dim=dec[\"hidden_dim\"],\n",
        "            num_heads=dec[\"num_heads\"],\n",
        "            max_len=seq_len\n",
        "        )\n",
        "\n",
        "    elif dec[\"type\"] == \"cnn\":\n",
        "        return CNNDecoder(\n",
        "            vocab_size=vocab,\n",
        "            latent_dim=latent,\n",
        "            seq_len=seq_len\n",
        "        )\n",
        "\n",
        "    elif dec[\"type\"] == \"transformer\":\n",
        "        return TransformerDecoder(\n",
        "            vocab_size=vocab,\n",
        "            latent_dim=latent,\n",
        "            emb_dim=dec[\"emb_dim\"],\n",
        "            n_heads=dec[\"num_heads\"],\n",
        "            n_layers=dec[\"num_layers\"],\n",
        "            max_len=seq_len\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown decoder type: {dec['type']}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CPIgRSkUuHIc"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#auto encoder building\n",
        "def build_model(cfg):\n",
        "    encoder = build_encoder(cfg)\n",
        "    decoder = build_decoder(cfg)\n",
        "\n",
        "    return DNAAutoEncoder(\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        seq_len=cfg[\"data\"][\"block_size\"]\n",
        "    )\n"
      ],
      "metadata": {
        "id": "YWIBCOcQuJn-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading fasta file\n",
        "def read_fasta(path):\n",
        "    sequence = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith('>'):\n",
        "                continue\n",
        "            sequence.append(line.upper())\n",
        "    dna = \"\".join(sequence)\n",
        "    validate_dna(dna)\n",
        "    return dna\n",
        "\n",
        "#validation\n",
        "def validate_dna(seq, allowed=None):\n",
        "    if allowed is None:\n",
        "        allowed = {'A', 'C', 'G', 'T', 'N'}\n",
        "    invalid = set(seq) - allowed\n",
        "    if invalid:\n",
        "        raise ValueError(f\"Invalid DNA symbols found: {invalid}\")\n",
        "\n",
        "#characters => integer tokens\n",
        "def encode_dna(seq):\n",
        "    return [stoi[ch] for ch in seq]\n",
        "\n",
        "#reverse\n",
        "def decode_dna(tokens):\n",
        "    return ''.join(itos[t] for t in tokens)\n",
        "\n",
        "#Split long DNA into fixed-length blocks\n",
        "def chunk_sequence(tokens, block_size, pad_token=PAD_TOKEN):\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), block_size):\n",
        "        block = tokens[i:i + block_size]\n",
        "        if len(block) < block_size:\n",
        "            block = block + [pad_token] * (block_size - len(block))\n",
        "        chunks.append(block)\n",
        "    return chunks\n",
        "\n",
        "#Convert Python list â†’ PyTorch tensor\n",
        "def prepare_dataset(blocks):\n",
        "    return torch.tensor(blocks, dtype=torch.long)\n",
        "\n",
        "#Measure how well the model reconstructs DNA\n",
        "def reconstruction_loss(logits, targets):\n",
        "    B, L, V = logits.shape\n",
        "\n",
        "    logits = logits.reshape(B * L, V)\n",
        "    targets = targets.reshape(B * L)\n",
        "\n",
        "    return F.cross_entropy(\n",
        "        logits,\n",
        "        targets,\n",
        "        ignore_index=PAD_TOKEN\n",
        "    )\n",
        "\n",
        "#Compute token-level reconstruction accuracy\n",
        "def reconstruction_accuracy(logits, targets):\n",
        "    \"\"\"\n",
        "    Computes token-level accuracy ignoring PAD tokens\n",
        "    \"\"\"\n",
        "    preds = torch.argmax(logits, dim=-1)   # (B, L)\n",
        "\n",
        "    mask = (targets != PAD_TOKEN)          # ignore padding\n",
        "    correct = (preds == targets) & mask\n",
        "\n",
        "    accuracy = correct.sum().float() / mask.sum().float()\n",
        "    return accuracy.item()\n",
        "\n",
        "#Compute base-by-base similarity between sequences\n",
        "def sequence_similarity(original, reconstructed):\n",
        "    matches = sum(o == r for o, r in zip(original, reconstructed))\n",
        "    return matches / len(original)\n",
        "\n",
        "#train\n",
        "def train_autoencoder(model, dataset, epochs, batch_size, lr):\n",
        "    model.to(device)\n",
        "    dataset = dataset.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    num_samples = dataset.size(0)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        perm = torch.randperm(num_samples)\n",
        "\n",
        "        total_loss = 0.0\n",
        "        total_acc = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            idx = perm[i:i + batch_size]\n",
        "            batch = dataset[idx]\n",
        "\n",
        "            logits = model(batch)\n",
        "            loss = reconstruction_loss(logits, batch)\n",
        "            acc = reconstruction_accuracy(logits, batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc\n",
        "            steps += 1\n",
        "            # print(num_samples,batch_size)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{epochs}] | \"\n",
        "            f\"Loss: {total_loss/steps:.4f} | \"\n",
        "            f\"Train Acc: {total_acc/steps:.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "usXTi2PRswrt"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving latent in a text file\n",
        "def save_latents_readable(model, dataset, path=\"latents.txt\"):\n",
        "    model.eval()\n",
        "    latents = []\n",
        "\n",
        "    with torch.no_grad(), open(path, \"w\") as f:\n",
        "        for i in range(dataset.size(0)):\n",
        "            x = dataset[i:i+1].to(device)\n",
        "            z = model.encoder(x).squeeze(0).cpu().tolist()\n",
        "            latents.append(z)\n",
        "\n",
        "            f.write(\",\".join(f\"{v:.6f}\" for v in z) + \"\\n\")\n",
        "\n",
        "    print(f\"Latents saved to {path}\")\n",
        "    return latents\n",
        "\n",
        "#saving latent in a .pt file\n",
        "def save_latents_binary(model, dataset, path=\"latents.pt\"):\n",
        "    model.eval()\n",
        "    latents = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(dataset.size(0)):\n",
        "            x = dataset[i:i+1].to(device)\n",
        "            z = model.encoder(x)\n",
        "            latents.append(z.cpu())\n",
        "\n",
        "    latents = torch.cat(latents, dim=0)  # (N, latent_dim)\n",
        "    torch.save(latents, path)\n",
        "    return latents\n",
        "\n",
        "# def compare_sizes(raw_dna, latent_txt_path):\n",
        "#     original_bytes = len(raw_dna)  # 1 char = 1 byte\n",
        "#     compressed_bytes = os.path.getsize(latent_txt_path)\n",
        "\n",
        "#     ratio = original_bytes / compressed_bytes\n",
        "\n",
        "#     print(f\"Original size   : {original_bytes} bytes\")\n",
        "#     print(f\"Compressed size : {compressed_bytes} bytes\")\n",
        "#     print(f\"Compression ratio: {ratio:.4f}\")\n",
        "\n",
        "    # return original_bytes, compressed_bytes, ratio\n",
        "\n",
        "#comparison of decoded sequence and original sequence\n",
        "def reconstruct_and_compare_safe(\n",
        "    model,\n",
        "    dataset,\n",
        "    raw_dna,\n",
        "    batch_size=16\n",
        "):\n",
        "    model.eval()\n",
        "    reconstructed_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, dataset.size(0), batch_size):\n",
        "            batch = dataset[i:i+batch_size].to(device)\n",
        "\n",
        "            logits = model(batch)                 # (B, L, V)\n",
        "            preds = torch.argmax(logits, dim=-1)  # (B, L)\n",
        "\n",
        "            for block in preds.cpu().tolist():\n",
        "                reconstructed_tokens.extend(block)\n",
        "\n",
        "    # Remove padding tokens beyond original length\n",
        "    reconstructed_tokens = reconstructed_tokens[:len(raw_dna)]\n",
        "\n",
        "    reconstructed_dna = decode_dna(reconstructed_tokens)\n",
        "    similarity = sequence_similarity(raw_dna, reconstructed_dna)\n",
        "\n",
        "    print(\"\\n===== RECONSTRUCTION RESULT =====\")\n",
        "    print(\"Original (first 200):\")\n",
        "    print(raw_dna[:200])\n",
        "\n",
        "    print(\"\\nReconstructed (first 200):\")\n",
        "    print(reconstructed_dna[:200])\n",
        "\n",
        "    print(f\"\\nSequence similarity: {similarity:.4f}\")\n",
        "\n",
        "    return reconstructed_dna, similarity\n",
        "\n",
        "#compression ratio for text file\n",
        "def compression_ratio_text(raw_dna, latent_txt_file):\n",
        "    \"\"\"\n",
        "    Compression ratio using readable text latents\n",
        "    \"\"\"\n",
        "    original_size = len(raw_dna)                 # bytes\n",
        "    compressed_size = os.path.getsize(latent_txt_file)\n",
        "\n",
        "    ratio = original_size / compressed_size\n",
        "\n",
        "    print(\"\\n===== COMPRESSION (TEXT LATENTS) =====\")\n",
        "    print(f\"Original size   : {original_size} bytes\")\n",
        "    print(f\"Compressed size : {compressed_size} bytes\")\n",
        "    print(f\"Compression ratio: {ratio:.4f}\")\n",
        "\n",
        "    return original_size, compressed_size, ratio\n",
        "\n",
        "#compression ratio for .pt file\n",
        "def compression_ratio_binary(raw_dna, latent_pt_file):\n",
        "    \"\"\"\n",
        "    Compression ratio using binary tensor latents\n",
        "    \"\"\"\n",
        "    original_size = len(raw_dna)                  # bytes\n",
        "    compressed_size = os.path.getsize(latent_pt_file)\n",
        "\n",
        "    ratio = original_size / compressed_size\n",
        "\n",
        "    print(\"\\n===== COMPRESSION (BINARY LATENTS) =====\")\n",
        "    print(f\"Original size   : {original_size} bytes\")\n",
        "    print(f\"Compressed size : {compressed_size} bytes\")\n",
        "    print(f\"Compression ratio: {ratio:.4f}\")\n",
        "\n",
        "    return original_size, compressed_size, ratio\n",
        "\n"
      ],
      "metadata": {
        "id": "bvzkFEOqc2Ug"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #data\n",
        "    dna_seq = read_fasta(\"AeCa .txt\")\n",
        "    dna_seq = dna_seq[:50000]\n",
        "    tokens = encode_dna(dna_seq)\n",
        "    blocks = chunk_sequence(tokens, CONFIG[\"data\"][\"block_size\"])\n",
        "    dataset = prepare_dataset(blocks)\n",
        "\n",
        "    #model\n",
        "    model = build_model(CONFIG).to(CONFIG[\"device\"])\n",
        "\n",
        "    #train\n",
        "    train_autoencoder(\n",
        "        model=model,\n",
        "        dataset=dataset,\n",
        "        epochs=CONFIG[\"training\"][\"epochs\"],\n",
        "        batch_size=CONFIG[\"training\"][\"batch_size\"],\n",
        "        lr=CONFIG[\"training\"][\"learning_rate\"]\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NQ_mLi4uZXz",
        "outputId": "01b69977-f6f1-4dd8-b406-d8ec02ef519d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30] | Loss: 1.4795 | Train Acc: 0.2618\n",
            "Epoch [2/30] | Loss: 1.3940 | Train Acc: 0.2633\n",
            "Epoch [3/30] | Loss: 1.3905 | Train Acc: 0.2609\n",
            "Epoch [4/30] | Loss: 1.3882 | Train Acc: 0.2628\n",
            "Epoch [5/30] | Loss: 1.3862 | Train Acc: 0.2681\n",
            "Epoch [6/30] | Loss: 1.3813 | Train Acc: 0.2784\n",
            "Epoch [7/30] | Loss: 1.3795 | Train Acc: 0.2795\n",
            "Epoch [8/30] | Loss: 1.3774 | Train Acc: 0.2876\n",
            "Epoch [9/30] | Loss: 1.3775 | Train Acc: 0.2879\n",
            "Epoch [10/30] | Loss: 1.3779 | Train Acc: 0.2950\n",
            "Epoch [11/30] | Loss: 1.3744 | Train Acc: 0.2997\n",
            "Epoch [12/30] | Loss: 1.3679 | Train Acc: 0.3167\n",
            "Epoch [13/30] | Loss: 1.3656 | Train Acc: 0.3202\n",
            "Epoch [14/30] | Loss: 1.3651 | Train Acc: 0.3182\n",
            "Epoch [15/30] | Loss: 1.3614 | Train Acc: 0.3213\n",
            "Epoch [16/30] | Loss: 1.3597 | Train Acc: 0.3222\n",
            "Epoch [17/30] | Loss: 1.3579 | Train Acc: 0.3270\n",
            "Epoch [18/30] | Loss: 1.3583 | Train Acc: 0.3235\n",
            "Epoch [19/30] | Loss: 1.3558 | Train Acc: 0.3275\n",
            "Epoch [20/30] | Loss: 1.3565 | Train Acc: 0.3273\n",
            "Epoch [21/30] | Loss: 1.3557 | Train Acc: 0.3279\n",
            "Epoch [22/30] | Loss: 1.3552 | Train Acc: 0.3290\n",
            "Epoch [23/30] | Loss: 1.3554 | Train Acc: 0.3282\n",
            "Epoch [24/30] | Loss: 1.3543 | Train Acc: 0.3299\n",
            "Epoch [25/30] | Loss: 1.3542 | Train Acc: 0.3315\n",
            "Epoch [26/30] | Loss: 1.3530 | Train Acc: 0.3309\n",
            "Epoch [27/30] | Loss: 1.3521 | Train Acc: 0.3322\n",
            "Epoch [28/30] | Loss: 1.3514 | Train Acc: 0.3314\n",
            "Epoch [29/30] | Loss: 1.3521 | Train Acc: 0.3321\n",
            "Epoch [30/30] | Loss: 1.3524 | Train Acc: 0.3314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== LATENT ANALYSIS =====\n",
        "\n",
        "\n",
        "latents = save_latents_readable(\n",
        "    model=model,\n",
        "    dataset=dataset,\n",
        "    path=\"latents.txt\"\n",
        ")\n",
        "\n",
        "# compare_sizes(dna_seq, \"latents.txt\")\n",
        "\n",
        "reconstructed_dna, similarity = reconstruct_and_compare_safe(\n",
        "    model=model,\n",
        "    dataset=dataset,\n",
        "    raw_dna=dna_seq,\n",
        "    batch_size=16\n",
        ")\n",
        "# ===== SAVE LATENTS =====\n",
        "save_latents_binary(model, dataset, \"latents.pt\")\n",
        "\n",
        "# ===== COMPRESSION =====\n",
        "compression_ratio_text(dna_seq, \"latents.txt\")\n",
        "compression_ratio_binary(dna_seq, \"latents.pt\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx3Gm2YfuULV",
        "outputId": "306b98dc-0e61-4796-de4d-3c2d97a24053"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latents saved to latents.txt\n",
            "\n",
            "===== RECONSTRUCTION RESULT =====\n",
            "Original (first 200):\n",
            "GCCGCCCCCATGGTCCATACGGTGTGCGAATACGGCGTGGCCCTCCTTACCCCATCCAGGCCTCTCTACGCCCCACTTGTCTATAGTGCCTTTCACGACCCTGGCCACAAGGTCGATCGCGTACTCCCAGGAGACAGGCTCTAGCCTTCCTCCGAACCTTACGAGGGGCTTTGTCAGCCTCCTCTCGCCTGCTATGTTCC\n",
            "\n",
            "Reconstructed (first 200):\n",
            "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
            "\n",
            "Sequence similarity: 0.3352\n",
            "\n",
            "===== COMPRESSION (TEXT LATENTS) =====\n",
            "Original size   : 50000 bytes\n",
            "Compressed size : 236640 bytes\n",
            "Compression ratio: 0.2113\n",
            "\n",
            "===== COMPRESSION (BINARY LATENTS) =====\n",
            "Original size   : 50000 bytes\n",
            "Compressed size : 101673 bytes\n",
            "Compression ratio: 0.4918\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 101673, 0.49177264367137785)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rjvg-dVNeB2j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}