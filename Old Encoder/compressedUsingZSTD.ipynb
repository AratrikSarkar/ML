{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/hNJwOSSu6ok2RDQzg3JG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AratrikSarkar/ML/blob/main/compressedUsingZSTD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_-462yomALq",
        "outputId": "d86f28aa-22e1-4a78-d4e2-64163910adc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | Loss 0.2041 | Acc 0.9111\n",
            "Epoch 2/50 | Loss 0.0250 | Acc 0.9919\n",
            "Epoch 3/50 | Loss 0.0138 | Acc 0.9958\n",
            "Epoch 4/50 | Loss 0.0105 | Acc 0.9970\n",
            "Epoch 5/50 | Loss 0.0075 | Acc 0.9979\n",
            "Epoch 6/50 | Loss 0.0058 | Acc 0.9984\n",
            "Epoch 7/50 | Loss 0.0042 | Acc 0.9989\n",
            "Epoch 8/50 | Loss 0.0042 | Acc 0.9989\n",
            "Epoch 9/50 | Loss 0.0042 | Acc 0.9988\n",
            "Epoch 10/50 | Loss 0.0040 | Acc 0.9989\n",
            "Epoch 11/50 | Loss 0.0035 | Acc 0.9991\n",
            "Epoch 12/50 | Loss 0.0025 | Acc 0.9994\n",
            "Epoch 13/50 | Loss 0.0022 | Acc 0.9994\n",
            "Epoch 14/50 | Loss 0.0042 | Acc 0.9989\n",
            "Epoch 15/50 | Loss 0.0019 | Acc 0.9996\n",
            "Epoch 16/50 | Loss 0.0027 | Acc 0.9993\n",
            "Epoch 17/50 | Loss 0.0021 | Acc 0.9995\n",
            "Epoch 18/50 | Loss 0.0028 | Acc 0.9993\n",
            "Epoch 19/50 | Loss 0.0023 | Acc 0.9994\n",
            "Epoch 20/50 | Loss 0.0024 | Acc 0.9994\n",
            "Epoch 21/50 | Loss 0.0023 | Acc 0.9995\n",
            "Epoch 22/50 | Loss 0.0012 | Acc 0.9997\n",
            "Epoch 23/50 | Loss 0.0011 | Acc 0.9998\n",
            "Epoch 24/50 | Loss 0.0019 | Acc 0.9996\n",
            "Epoch 25/50 | Loss 0.0021 | Acc 0.9995\n",
            "Epoch 26/50 | Loss 0.0016 | Acc 0.9997\n",
            "Epoch 27/50 | Loss 0.0007 | Acc 0.9998\n",
            "Epoch 28/50 | Loss 0.0018 | Acc 0.9996\n",
            "Epoch 29/50 | Loss 0.0022 | Acc 0.9995\n",
            "Epoch 30/50 | Loss 0.0010 | Acc 0.9998\n",
            "Epoch 31/50 | Loss 0.0021 | Acc 0.9995\n",
            "Epoch 32/50 | Loss 0.0021 | Acc 0.9995\n",
            "Epoch 33/50 | Loss 0.0006 | Acc 0.9999\n",
            "Epoch 34/50 | Loss 0.0016 | Acc 0.9997\n",
            "Epoch 35/50 | Loss 0.0018 | Acc 0.9996\n",
            "Epoch 36/50 | Loss 0.0020 | Acc 0.9995\n",
            "Epoch 37/50 | Loss 0.0000 | Acc 1.0000\n",
            "Epoch 38/50 | Loss 0.0010 | Acc 0.9998\n",
            "Epoch 39/50 | Loss 0.0019 | Acc 0.9996\n",
            "Epoch 40/50 | Loss 0.0005 | Acc 0.9999\n",
            "Epoch 41/50 | Loss 0.0013 | Acc 0.9997\n",
            "Epoch 42/50 | Loss 0.0017 | Acc 0.9996\n",
            "Epoch 43/50 | Loss 0.0023 | Acc 0.9995\n",
            "Epoch 44/50 | Loss 0.0012 | Acc 0.9997\n",
            "Epoch 45/50 | Loss 0.0020 | Acc 0.9996\n",
            "Epoch 46/50 | Loss 0.0014 | Acc 0.9997\n",
            "Epoch 47/50 | Loss 0.0016 | Acc 0.9996\n",
            "Epoch 48/50 | Loss 0.0019 | Acc 0.9996\n",
            "Epoch 49/50 | Loss 0.0019 | Acc 0.9996\n",
            "Epoch 50/50 | Loss 0.0016 | Acc 0.9997\n",
            "\n",
            "TEST FILE: YeMi\n",
            "Similarity: 0.6732348111658456\n",
            "Original size: 73689\n",
            "latent.pt size: 75269\n",
            "latent.zst size: 48160\n",
            "\n",
            "TEST FILE: AeCa\n",
            "Similarity: 0.7023501630495895\n",
            "Original size: 542473\n",
            "latent.pt size: 544069\n",
            "latent.zst size: 297783\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# ================= CONFIG =================\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BLOCK_SIZE = 8\n",
        "LATENT_DIM = 8\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "LR = 5e-4\n",
        "\n",
        "DNA_VOCAB = ['A','C','G','T','N']\n",
        "stoi = {c:i for i,c in enumerate(DNA_VOCAB)}\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "PAD = stoi['N']\n",
        "\n",
        "# ================= DATA =================\n",
        "def read_fasta(path):\n",
        "    seq = []\n",
        "    with open(path) as f:\n",
        "        for l in f:\n",
        "            l = l.strip().upper()\n",
        "            if l and not l.startswith(\">\"):\n",
        "                seq.append(l)\n",
        "    return \"\".join(seq)\n",
        "\n",
        "def encode_dna(seq):\n",
        "    return [stoi[c] for c in seq]\n",
        "\n",
        "def decode_dna(tokens):\n",
        "    return \"\".join(itos[int(t)] for t in tokens)\n",
        "\n",
        "def chunk(tokens, size):\n",
        "    out = []\n",
        "    for i in range(0, len(tokens), size):\n",
        "        b = tokens[i:i+size]\n",
        "        if len(b) < size:\n",
        "            b += [PAD] * (size - len(b))\n",
        "        out.append(b)\n",
        "    return out\n",
        "\n",
        "# ================= MODEL =================\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(len(DNA_VOCAB), 64)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(64,128,7,padding=3), nn.ReLU(),\n",
        "            nn.Conv1d(128,256,7,padding=3), nn.ReLU(),\n",
        "            nn.Conv1d(256,256,7,padding=3), nn.ReLU()\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(256, LATENT_DIM)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x).transpose(1,2)\n",
        "        x = self.conv(x)\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        return self.fc(x)\n",
        "\n",
        "class CNNDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(LATENT_DIM,256)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(256,256,7,padding=3), nn.ReLU(),\n",
        "            nn.Conv1d(256,256,7,padding=3), nn.ReLU(),\n",
        "            nn.Conv1d(256,128,7,padding=3), nn.ReLU(),\n",
        "            nn.Conv1d(128,len(DNA_VOCAB),1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, seq_len):\n",
        "        x = self.fc(z).unsqueeze(-1).repeat(1,1,seq_len)\n",
        "        x = self.conv(x)\n",
        "        return x.transpose(1,2)\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.enc = CNNEncoder()\n",
        "        self.dec = CNNDecoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.enc(x)\n",
        "        return self.dec(z, x.size(1))\n",
        "\n",
        "# ================= LOSS =================\n",
        "def loss_fn(logits, targets):\n",
        "    return F.cross_entropy(\n",
        "        logits.reshape(-1, logits.size(-1)),\n",
        "        targets.reshape(-1),\n",
        "        ignore_index=PAD\n",
        "    )\n",
        "\n",
        "def accuracy_fn(logits, targets):\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "    mask = targets != PAD\n",
        "    correct = (preds == targets) & mask\n",
        "    return (correct.sum().float() / mask.sum().float()).item()\n",
        "\n",
        "# ================= PATHS =================\n",
        "TRAIN_DIR = \"data\"\n",
        "TEST_DIR  = \"testing\"\n",
        "OUT_DIR   = \"outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ================= LOAD TRAIN DATA =================\n",
        "train_tokens = []\n",
        "for f in os.listdir(TRAIN_DIR):\n",
        "    p = os.path.join(TRAIN_DIR, f)\n",
        "    if os.path.isfile(p):\n",
        "        train_tokens.extend(encode_dna(read_fasta(p)))\n",
        "\n",
        "train_data = torch.tensor(chunk(train_tokens, BLOCK_SIZE), dtype=torch.long).to(DEVICE)\n",
        "\n",
        "# ================= TRAIN =================\n",
        "model = AutoEncoder().to(DEVICE)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "    model.train()\n",
        "    perm = torch.randperm(train_data.size(0))\n",
        "    loss_sum = acc_sum = steps = 0\n",
        "\n",
        "    for i in range(0, train_data.size(0), BATCH_SIZE):\n",
        "        batch = train_data[perm[i:i+BATCH_SIZE]]\n",
        "        out = model(batch)\n",
        "        loss = loss_fn(out, batch)\n",
        "        acc  = accuracy_fn(out, batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        loss_sum += loss.item()\n",
        "        acc_sum  += acc\n",
        "        steps += 1\n",
        "\n",
        "    print(f\"Epoch {e+1}/{EPOCHS} | Loss {loss_sum/steps:.4f} | Acc {acc_sum/steps:.4f}\")\n",
        "\n",
        "# ================= TEST + SAVE FILES =================\n",
        "model.eval()\n",
        "\n",
        "for fname in os.listdir(TEST_DIR):\n",
        "    path = os.path.join(TEST_DIR, fname)\n",
        "    if not os.path.isfile(path):\n",
        "        continue\n",
        "\n",
        "    base = os.path.splitext(fname)[0]\n",
        "    dna = read_fasta(path)\n",
        "    tokens = encode_dna(dna)\n",
        "\n",
        "    # ---- encoded.txt ----\n",
        "    with open(f\"{OUT_DIR}/{base}_encoded.txt\", \"w\") as f:\n",
        "        f.write(\" \".join(map(str, tokens)))\n",
        "\n",
        "    blocks = chunk(tokens, BLOCK_SIZE)\n",
        "    test_data = torch.tensor(blocks, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "    # ---- Encode latents ----\n",
        "    latents = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(test_data.size(0)):\n",
        "            latents.append(model.enc(test_data[i:i+1]).cpu())\n",
        "    latents = torch.cat(latents, 0)\n",
        "\n",
        "    latents_q = (latents * 127).round().clamp(-128,127).to(torch.int8)\n",
        "\n",
        "    # ---- latent.pt ----\n",
        "    latent_pt = f\"{OUT_DIR}/{base}_latent.pt\"\n",
        "    torch.save(latents_q, latent_pt)\n",
        "\n",
        "    # ---- zstd compression ----\n",
        "    latent_zst = latent_pt + \".zst\"\n",
        "    subprocess.run([\"zstd\", \"-19\", latent_pt, \"-o\", latent_zst], check=True)\n",
        "\n",
        "    # ---- latent.txt ----\n",
        "    with open(f\"{OUT_DIR}/{base}_latent.txt\", \"w\") as f:\n",
        "        for row in (latents_q.float()/127):\n",
        "            f.write(\",\".join(f\"{v:.6f}\" for v in row.tolist()) + \"\\n\")\n",
        "\n",
        "    # ---- decompress for decoding ----\n",
        "    subprocess.run([\"zstd\", \"-d\", \"-f\", latent_zst, \"-o\", latent_pt], check=True)\n",
        "    loaded = torch.load(latent_pt).float() / 127.0\n",
        "\n",
        "    # ---- reconstruct ----\n",
        "    recon = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, loaded.size(0), BATCH_SIZE):\n",
        "            out = model.dec(loaded[i:i+BATCH_SIZE].to(DEVICE), BLOCK_SIZE)\n",
        "            preds = torch.argmax(out, -1).cpu().reshape(-1)\n",
        "            recon.extend(preds.tolist())\n",
        "\n",
        "    recon = recon[:len(tokens)]\n",
        "    recon_text = decode_dna(recon)\n",
        "\n",
        "    # ---- reconstructed.txt ----\n",
        "    with open(f\"{OUT_DIR}/{base}_reconstructed.txt\", \"w\") as f:\n",
        "        f.write(recon_text)\n",
        "\n",
        "    # ---- metrics ----\n",
        "    similarity = sum(a==b for a,b in zip(dna,recon_text)) / len(dna)\n",
        "\n",
        "    print(f\"\\nTEST FILE: {fname}\")\n",
        "    print(\"Similarity:\", similarity)\n",
        "    print(\"Original size:\", len(dna))\n",
        "    print(\"latent.pt size:\", os.path.getsize(latent_pt))\n",
        "    print(\"latent.zst size:\", os.path.getsize(latent_zst))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y zstd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCYaJT10dEwk",
        "outputId": "6cbfc700-eba2-4939-9f9f-aa7ada5c0ae8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  zstd\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 603 kB of archives.\n",
            "After this operation, 1,695 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 zstd amd64 1.4.8+dfsg-3build1 [603 kB]\n",
            "Fetched 603 kB in 0s (3,091 kB/s)\n",
            "Selecting previously unselected package zstd.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
            "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
            "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hr2lnzFVn8mr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}